{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40941f2-afb0-48b1-8173-50673ebff6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7a65062-72ba-40f0-ad0c-e37ed36df9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from dataset_preprocessing import TokenInfo\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d376ae97-4ec1-46d5-b640-566545c2e21d",
   "metadata": {},
   "source": [
    "## Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7943d9f-9fa3-4ae3-8e5b-2b8978c909a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_importances():\n",
    "    # print(\"this is wrong\")\n",
    "    dir = \"./new_importances_data\"\n",
    "    imp_files = os.listdir(dir)\n",
    "    imp_files = [file for file in imp_files if file.endswith(\".pkl\")]\n",
    "    importances = {}\n",
    "    for imp_file in tqdm(imp_files):\n",
    "        importances.update(pd.read_pickle(f\"{dir}/{imp_file}\"))\n",
    "    return importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92b949f6-b4f0-420a-b379-bdcb27600d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imps = get_importances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6aee5d8-b934-4388-a280-226e823cfc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_imporances(importances):\n",
    "    avg_imps = [torch.zeros_like(imp) for imp in list(importances.values())[0]]\n",
    "    for token, imps in tqdm(importances.items()):\n",
    "        for i, layer_imps in enumerate(imps):\n",
    "            avg_imps[i] += layer_imps / len(importances)\n",
    "    # TODO think harder about averaging method\n",
    "    return avg_imps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b49a204-d574-4c49-a615-f6cbe0d68103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg_importances = get_avg_imporances(imps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38b9cea4-4361-4750-935a-c313302ba106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.to_pickle(avg_importances, \"./avg_importances.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd7daa4f-5525-4bc6-ab49-df4e159f3449",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_importances = pd.read_pickle(\"./avg_importances.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f94d76e-fa17-4f7d-b1f6-cdad75876469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(avg_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4532c3-06f6-4353-b261-92acee331bbc",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41b0cc63-e94b-4d1e-aff7-ea5c7310e5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"microsoft/phi-1_5\"\n",
    "model_revision = \"349cf8b5e81fd5f791d1740da5de1313a0419bbd\" # latest as of feb 1st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a84556f1-3c37-4283-bbdb-7d57b2f8d4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "434047b0-2814-4a7d-8b36-26bb8cea6ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50295"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = tokenizer.get_vocab()\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0188674e-b98b-4880-8ea1-f5c7793d6a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.decode(token_info.get_prefixes(top_tokens[1000][0], 9, 10)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b379b1d8-0ab9-44df-83ed-089795112f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    revision=model_revision,\n",
    "    trust_remote_code=True,\n",
    "    # be careful with this?\n",
    "    # torch_dtype=torch.float16,\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29bb579-da14-4e34-b37f-eba117b1e816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c698778-7f3b-4cf8-8bfe-6b0c54849ca2",
   "metadata": {},
   "source": [
    "## Prune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9250468a-c3e1-498a-8799-bbbbc763f086",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prunners import prune_mlps_individually\n",
    "from importances import get_mlps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e11be390-99df-4b99-9e18-25255d3d269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlps = get_mlps(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "207a1fcd-edaf-43e2-a52d-ba4fc39b7c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 24)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mlps), len(avg_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0810f24f-cb59-4c40-9b66-588c669bb2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_importances = dict(zip(mlps, avg_importances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c222ae1-5dcd-4801-96dc-0ebdfd10666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_mlps_individually(avg_importances, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b96da5cc-9b2c-44e1-8b38-ef0592767ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiForCausalLM(\n",
       "  (model): PhiModel(\n",
       "    (embed_tokens): Embedding(51200, 2048)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x PhiDecoderLayer(\n",
       "        (self_attn): PhiAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (rotary_emb): PhiRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): PhiMLP(\n",
       "          (activation_fn): NewGELUActivation()\n",
       "          (fc1): Linear(in_features=2048, out_features=6554, bias=True)\n",
       "          (fc2): Linear(in_features=6554, out_features=2048, bias=True)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (final_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=51200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174b2b6f-45a6-4329-bdd3-171d70d2038f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "028756c1-b952-49e2-acad-476c47285710",
   "metadata": {},
   "source": [
    "## Replace model modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "129af7bc-6a05-4d98-b0e5-ebe9ea37159a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/research/robgarct/.conda/envs/cs224n-pip3/lib/python3.11/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from experts import Experts, EmbeddingTokenIdxTracker, mark_only_adapters_as_trainable, prepare_as_if_peft_model, prepare_model_for_gradient_checkpointing\n",
    "from importances import get_mlps\n",
    "from post_training import get_lora_config, get_training_arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1694358-33a6-4323-be62-fb5c6660b100",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = get_lora_config()\n",
    "training_arguments = get_training_arguments(\"./tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b3db38f-f332-4fb9-8cbe-da4578d71d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = prepare_as_if_peft_model(model, training_arguments, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a389136c-d30f-486e-aa89-42d47ded3da1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c60286a8-da58-451c-ba60-5539e6f37588",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_tokens_new = EmbeddingTokenIdxTracker(model.get_submodule(\"model\").get_submodule(\"embed_tokens\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c3be121-252c-4076-85f4-1149821f4a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layers(model):\n",
    "    return model.get_submodule(\"model\").get_submodule(\"layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2371bb5-6bd5-4832-94de-07e58a7997f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = get_layers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc6cab4a-a81d-415d-850c-544bab24d128",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_submodule(\"model\").embed_tokens = embed_tokens_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a9f488d-2ea2-4e47-bafe-cee9856f4c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, layer in enumerate(layers):\n",
    "    layer.mlp = Experts(\n",
    "        model,\n",
    "        layer.mlp,\n",
    "        lora_config,\n",
    "        i,\n",
    "        embed_tokens_new.idx_tracker,\n",
    "        layer.mlp.config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82509117-64fd-410c-86bc-6a9145151c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "mark_only_adapters_as_trainable(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "44f81670-8720-40f8-871e-88c97cd630e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiForCausalLM(\n",
       "  (model): PhiModel(\n",
       "    (embed_tokens): EmbeddingTokenIdxTracker(\n",
       "      (embed): Embedding(51200, 2048)\n",
       "    )\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x PhiDecoderLayer(\n",
       "        (self_attn): PhiAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (rotary_emb): PhiRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Experts(\n",
       "          (activation_fn): NewGELUActivation()\n",
       "          (cluster_router): ClusterRouter()\n",
       "          (experts_fc1): ModuleList(\n",
       "            (0-7): 8 x lora.Linear(\n",
       "              (base_layer): Linear(in_features=2048, out_features=6554, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=6554, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "            )\n",
       "          )\n",
       "          (experts_fc2): ModuleList(\n",
       "            (0-7): 8 x lora.Linear(\n",
       "              (base_layer): Linear(in_features=6554, out_features=2048, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=6554, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (final_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=51200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_model_for_gradient_checkpointing(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d5aa50-ccb0-402f-b8b9-9a30a51ec311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8482f4a-8430-4d3e-aa9d-8f7b0def7bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\"hi this is an example\", \"hi this is an example\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3067f38a-ffb5-471c-8ff5-7f1770837f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = torch.tensor(tokenizer.encode(examples)).view(-1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84c33521-8b1a-41cc-8ebc-30189ce05c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiForCausalLM(\n",
       "  (model): PhiModel(\n",
       "    (embed_tokens): EmbeddingTokenIdxTracker(\n",
       "      (embed): Embedding(51200, 2048)\n",
       "    )\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x PhiDecoderLayer(\n",
       "        (self_attn): PhiAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (rotary_emb): PhiRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Experts(\n",
       "          (activation_fn): NewGELUActivation()\n",
       "          (cluster_router): ClusterRouter()\n",
       "          (experts_fc1): ModuleList(\n",
       "            (0-7): 8 x lora.Linear(\n",
       "              (base_layer): Linear(in_features=2048, out_features=6554, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=6554, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "            )\n",
       "          )\n",
       "          (experts_fc2): ModuleList(\n",
       "            (0-7): 8 x lora.Linear(\n",
       "              (base_layer): Linear(in_features=6554, out_features=2048, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=6554, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (final_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=51200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "043ab127-fe1b-495f-847e-d5e84a621f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = model(examples.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0229563-96d2-4bbe-a23b-8df5c92c822d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1469b844-b02a-4b7b-98ca-28c5759cd9a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3eb57e0-ca3f-49ed-b43b-a5874dc6d583",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5fa8dd8-6987-4888-8583-592da2e56506",
   "metadata": {},
   "outputs": [],
   "source": [
    "from post_training import get_lora_config, get_training_arguments\n",
    "from dataset import get_baseline_dataset\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "import transformers\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "318fb266-d940-4daa-b643-07247fbc9e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading pickle\n"
     ]
    }
   ],
   "source": [
    "dataset = get_baseline_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "949f2b82-94fc-4ec3-8728-f2f5ae375a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 60\n",
    "# micro_batch_size = 6\n",
    "# gradient_accumulation_steps = batch_size // micro_batch_size\n",
    "# training_arguments = transformers.TrainingArguments(\n",
    "#     per_device_train_batch_size=micro_batch_size,\n",
    "#     gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "#     warmup_steps=100,\n",
    "#     num_train_epochs=2,\n",
    "#     learning_rate=1e-4,\n",
    "#     fp16=True,\n",
    "#     logging_steps=10,\n",
    "#     logging_first_step=True,\n",
    "#     # optim=torch.optim,\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     save_strategy=\"steps\",\n",
    "#     eval_steps=100,\n",
    "#     save_steps=200,\n",
    "#     output_dir=\"./tmp\",\n",
    "#     save_total_limit=20,\n",
    "#     load_best_model_at_end=True,\n",
    "#     ddp_find_unused_parameters=None,\n",
    "#     group_by_length=False,\n",
    "#     # metric_for_best_model=\"{}_loss\".format(args.data_path),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "355927ec-3ea2-4b64-9022-d8a5bbed156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ef8bc33-998c-4c3c-a979-e05d0d6938dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a4567db018467fab7702cd844e9408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce18d4b9b2b74abdbf3c72eb6e43ac8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# Setup model for training\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Setup tokenizer for trainign\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "train_data, eval_data = dataset[\"train\"], dataset[\"test\"]\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    # peft_config=lora_config,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=False,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=1024, # tweak this\n",
    "    # TODO: think harder about the datacollator\n",
    "    # data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "    #     tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "    # ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "91f25bee-d634-4262-b712-27b4d6384d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CodeGenTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 43:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 3.416672468185425,\n",
       " 'eval_runtime': 213.6344,\n",
       " 'eval_samples_per_second': 9.362,\n",
       " 'eval_steps_per_second': 1.17}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8c0d8e55-1f1d-4725-9d04-9c28ea702b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1601' max='1666' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1601/1666 10:56:21 < 26:40, 0.04 it/s, Epoch 1.92/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.236000</td>\n",
       "      <td>3.190022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.157300</td>\n",
       "      <td>3.144821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.170600</td>\n",
       "      <td>3.128407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.086900</td>\n",
       "      <td>3.118995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.062500</td>\n",
       "      <td>3.111629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.140000</td>\n",
       "      <td>3.106526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.088400</td>\n",
       "      <td>3.103107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.151600</td>\n",
       "      <td>3.100001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.052700</td>\n",
       "      <td>3.098915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.042100</td>\n",
       "      <td>3.098850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>3.082300</td>\n",
       "      <td>3.097995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>3.040300</td>\n",
       "      <td>3.097095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>3.122000</td>\n",
       "      <td>3.096202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>3.084400</td>\n",
       "      <td>3.095358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.051600</td>\n",
       "      <td>3.094743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>3.050800</td>\n",
       "      <td>3.094116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removed shared tensor {'model.layers.20.mlp.experts_fc2.4.base_layer.weight', 'model.layers.15.mlp.experts_fc2.4.base_layer.bias', 'model.layers.5.mlp.experts_fc2.7.base_layer.bias', 'model.layers.15.mlp.experts_fc1.4.base_layer.weight', 'model.layers.0.mlp.experts_fc2.2.base_layer.bias', 'model.layers.8.mlp.experts_fc2.7.base_layer.weight', 'model.layers.3.mlp.experts_fc2.1.base_layer.bias', 'model.layers.7.mlp.experts_fc2.1.base_layer.bias', 'model.layers.7.mlp.experts_fc2.5.base_layer.weight', 'model.layers.15.mlp.experts_fc2.1.base_layer.bias', 'model.layers.22.mlp.experts_fc1.3.base_layer.weight', 'model.layers.9.mlp.experts_fc1.6.base_layer.weight', 'model.layers.12.mlp.experts_fc1.5.base_layer.weight', 'model.layers.17.mlp.experts_fc2.5.base_layer.weight', 'model.layers.13.mlp.experts_fc1.1.base_layer.bias', 'model.layers.13.mlp.experts_fc2.7.base_layer.weight', 'model.layers.10.mlp.experts_fc2.1.base_layer.weight', 'model.layers.18.mlp.experts_fc2.3.base_layer.bias', 'model.layers.9.mlp.experts_fc1.5.base_layer.bias', 'model.layers.17.mlp.experts_fc2.4.base_layer.bias', 'model.layers.21.mlp.experts_fc1.4.base_layer.weight', 'model.layers.0.mlp.experts_fc1.4.base_layer.weight', 'model.layers.6.mlp.experts_fc2.6.base_layer.weight', 'model.layers.16.mlp.experts_fc2.6.base_layer.weight', 'model.layers.16.mlp.experts_fc2.4.base_layer.bias', 'model.layers.18.mlp.experts_fc1.7.base_layer.weight', 'model.layers.22.mlp.experts_fc2.2.base_layer.weight', 'model.layers.13.mlp.experts_fc1.6.base_layer.bias', 'model.layers.7.mlp.experts_fc2.2.base_layer.bias', 'model.layers.7.mlp.experts_fc1.7.base_layer.bias', 'model.layers.20.mlp.experts_fc2.6.base_layer.weight', 'model.layers.4.mlp.experts_fc2.4.base_layer.weight', 'model.layers.4.mlp.experts_fc2.7.base_layer.bias', 'model.layers.8.mlp.experts_fc1.1.base_layer.weight', 'model.layers.12.mlp.experts_fc1.2.base_layer.weight', 'model.layers.10.mlp.experts_fc1.5.base_layer.bias', 'model.layers.7.mlp.experts_fc1.2.base_layer.bias', 'model.layers.7.mlp.experts_fc1.3.base_layer.bias', 'model.layers.19.mlp.experts_fc1.2.base_layer.bias', 'model.layers.16.mlp.experts_fc1.1.base_layer.bias', 'model.layers.17.mlp.experts_fc1.7.base_layer.bias', 'model.layers.8.mlp.experts_fc2.4.base_layer.bias', 'model.layers.9.mlp.experts_fc2.5.base_layer.weight', 'model.layers.0.mlp.experts_fc2.5.base_layer.bias', 'model.layers.9.mlp.experts_fc1.2.base_layer.bias', 'model.layers.13.mlp.experts_fc2.6.base_layer.bias', 'model.layers.13.mlp.experts_fc2.2.base_layer.weight', 'model.layers.14.mlp.experts_fc1.5.base_layer.weight', 'model.layers.0.mlp.experts_fc1.7.base_layer.weight', 'model.layers.6.mlp.experts_fc2.6.base_layer.bias', 'model.layers.5.mlp.experts_fc1.1.base_layer.weight', 'model.layers.22.mlp.experts_fc2.1.base_layer.bias', 'model.layers.21.mlp.experts_fc1.3.base_layer.bias', 'model.layers.3.mlp.experts_fc2.7.base_layer.bias', 'model.layers.9.mlp.experts_fc2.3.base_layer.weight', 'model.layers.9.mlp.experts_fc1.6.base_layer.bias', 'model.layers.7.mlp.experts_fc2.6.base_layer.weight', 'model.layers.1.mlp.experts_fc1.2.base_layer.bias', 'model.layers.21.mlp.experts_fc2.4.base_layer.bias', 'model.layers.12.mlp.experts_fc1.4.base_layer.weight', 'model.layers.6.mlp.experts_fc1.1.base_layer.weight', 'model.layers.0.mlp.experts_fc1.3.base_layer.weight', 'model.layers.15.mlp.experts_fc2.7.base_layer.weight', 'model.layers.11.mlp.experts_fc1.3.base_layer.weight', 'model.layers.7.mlp.experts_fc1.4.base_layer.bias', 'model.layers.8.mlp.experts_fc1.3.base_layer.weight', 'model.layers.11.mlp.experts_fc1.3.base_layer.bias', 'model.layers.11.mlp.experts_fc2.6.base_layer.weight', 'model.layers.0.mlp.experts_fc1.1.base_layer.bias', 'model.layers.20.mlp.experts_fc1.1.base_layer.bias', 'model.layers.14.mlp.experts_fc1.3.base_layer.bias', 'model.layers.11.mlp.experts_fc2.3.base_layer.weight', 'model.layers.18.mlp.experts_fc2.3.base_layer.weight', 'model.layers.23.mlp.experts_fc1.6.base_layer.weight', 'model.layers.22.mlp.experts_fc2.5.base_layer.bias', 'model.layers.16.mlp.experts_fc2.7.base_layer.weight', 'model.layers.17.mlp.experts_fc1.3.base_layer.weight', 'model.layers.13.mlp.experts_fc1.3.base_layer.weight', 'model.layers.3.mlp.experts_fc1.2.base_layer.bias', 'model.layers.6.mlp.experts_fc2.7.base_layer.weight', 'model.layers.8.mlp.experts_fc1.6.base_layer.bias', 'model.layers.16.mlp.experts_fc1.3.base_layer.bias', 'model.layers.10.mlp.experts_fc1.7.base_layer.bias', 'model.layers.18.mlp.experts_fc1.2.base_layer.bias', 'model.layers.2.mlp.experts_fc2.5.base_layer.weight', 'model.layers.22.mlp.experts_fc2.3.base_layer.bias', 'model.layers.15.mlp.experts_fc1.6.base_layer.bias', 'model.layers.1.mlp.experts_fc1.4.base_layer.bias', 'model.layers.4.mlp.experts_fc1.2.base_layer.weight', 'model.layers.4.mlp.experts_fc2.5.base_layer.weight', 'model.layers.21.mlp.experts_fc1.3.base_layer.weight', 'model.layers.15.mlp.experts_fc2.4.base_layer.weight', 'model.layers.23.mlp.experts_fc2.7.base_layer.weight', 'model.layers.18.mlp.experts_fc2.6.base_layer.bias', 'model.layers.1.mlp.experts_fc1.7.base_layer.weight', 'model.layers.18.mlp.experts_fc2.1.base_layer.weight', 'model.layers.7.mlp.experts_fc1.5.base_layer.weight', 'model.layers.19.mlp.experts_fc2.5.base_layer.bias', 'model.layers.16.mlp.experts_fc1.6.base_layer.bias', 'model.layers.1.mlp.experts_fc1.4.base_layer.weight', 'model.layers.19.mlp.experts_fc1.7.base_layer.weight', 'model.layers.2.mlp.experts_fc1.6.base_layer.bias', 'model.layers.3.mlp.experts_fc2.4.base_layer.weight', 'model.layers.10.mlp.experts_fc2.5.base_layer.weight', 'model.layers.19.mlp.experts_fc2.2.base_layer.weight', 'model.layers.21.mlp.experts_fc2.7.base_layer.weight', 'model.layers.7.mlp.experts_fc2.3.base_layer.bias', 'model.layers.14.mlp.experts_fc2.2.base_layer.bias', 'model.layers.15.mlp.experts_fc2.1.base_layer.weight', 'model.layers.2.mlp.experts_fc1.1.base_layer.weight', 'model.layers.13.mlp.experts_fc1.1.base_layer.weight', 'model.layers.1.mlp.experts_fc2.4.base_layer.bias', 'model.layers.18.mlp.experts_fc1.6.base_layer.bias', 'model.layers.11.mlp.experts_fc2.1.base_layer.bias', 'model.layers.1.mlp.experts_fc1.6.base_layer.bias', 'model.layers.22.mlp.experts_fc1.7.base_layer.bias', 'model.layers.7.mlp.experts_fc1.7.base_layer.weight', 'model.layers.6.mlp.experts_fc1.2.base_layer.weight', 'model.layers.17.mlp.experts_fc2.4.base_layer.weight', 'model.layers.6.mlp.experts_fc1.3.base_layer.bias', 'model.layers.8.mlp.experts_fc2.6.base_layer.weight', 'model.layers.19.mlp.experts_fc1.7.base_layer.bias', 'model.layers.21.mlp.experts_fc1.6.base_layer.bias', 'model.layers.17.mlp.experts_fc1.4.base_layer.bias', 'model.layers.13.mlp.experts_fc1.5.base_layer.weight', 'model.layers.21.mlp.experts_fc2.6.base_layer.bias', 'model.layers.9.mlp.experts_fc1.2.base_layer.weight', 'model.layers.4.mlp.experts_fc1.7.base_layer.bias', 'model.layers.18.mlp.experts_fc2.5.base_layer.weight', 'model.layers.8.mlp.experts_fc1.3.base_layer.bias', 'model.layers.23.mlp.experts_fc2.5.base_layer.weight', 'model.layers.2.mlp.experts_fc1.3.base_layer.weight', 'model.layers.10.mlp.experts_fc2.6.base_layer.bias', 'model.layers.20.mlp.experts_fc2.5.base_layer.bias', 'model.layers.3.mlp.experts_fc2.2.base_layer.bias', 'model.layers.3.mlp.experts_fc2.7.base_layer.weight', 'model.layers.18.mlp.experts_fc1.5.base_layer.weight', 'model.layers.3.mlp.experts_fc1.2.base_layer.weight', 'model.layers.19.mlp.experts_fc2.1.base_layer.weight', 'model.layers.6.mlp.experts_fc2.7.base_layer.bias', 'model.layers.22.mlp.experts_fc1.4.base_layer.weight', 'model.layers.22.mlp.experts_fc2.4.base_layer.bias', 'model.layers.2.mlp.experts_fc1.6.base_layer.weight', 'model.layers.1.mlp.experts_fc1.2.base_layer.weight', 'model.layers.1.mlp.experts_fc1.6.base_layer.weight', 'model.layers.15.mlp.experts_fc1.5.base_layer.weight', 'model.layers.21.mlp.experts_fc1.2.base_layer.weight', 'model.layers.21.mlp.experts_fc1.6.base_layer.weight', 'model.layers.4.mlp.experts_fc1.3.base_layer.bias', 'model.layers.21.mlp.experts_fc1.1.base_layer.weight', 'model.layers.5.mlp.experts_fc1.5.base_layer.weight', 'model.layers.6.mlp.experts_fc1.2.base_layer.bias', 'model.layers.18.mlp.experts_fc2.5.base_layer.bias', 'model.layers.9.mlp.experts_fc1.7.base_layer.weight', 'model.layers.11.mlp.experts_fc1.6.base_layer.bias', 'model.layers.16.mlp.experts_fc1.3.base_layer.weight', 'model.layers.23.mlp.experts_fc2.3.base_layer.bias', 'model.layers.4.mlp.experts_fc2.1.base_layer.bias', 'model.layers.18.mlp.experts_fc1.7.base_layer.bias', 'model.layers.8.mlp.experts_fc2.3.base_layer.weight', 'model.layers.19.mlp.experts_fc1.2.base_layer.weight', 'model.layers.4.mlp.experts_fc2.7.base_layer.weight', 'model.layers.10.mlp.experts_fc2.2.base_layer.weight', 'model.layers.5.mlp.experts_fc2.2.base_layer.weight', 'model.layers.9.mlp.experts_fc2.1.base_layer.bias', 'model.layers.8.mlp.experts_fc2.5.base_layer.weight', 'model.layers.13.mlp.experts_fc2.6.base_layer.weight', 'model.layers.18.mlp.experts_fc1.4.base_layer.weight', 'model.layers.2.mlp.experts_fc1.3.base_layer.bias', 'model.layers.0.mlp.experts_fc2.7.base_layer.weight', 'model.layers.11.mlp.experts_fc1.1.base_layer.weight', 'model.layers.3.mlp.experts_fc2.6.base_layer.bias', 'model.layers.19.mlp.experts_fc2.5.base_layer.weight', 'model.layers.14.mlp.experts_fc1.5.base_layer.bias', 'model.layers.5.mlp.experts_fc1.6.base_layer.bias', 'model.layers.16.mlp.experts_fc1.2.base_layer.bias', 'model.layers.16.mlp.experts_fc1.1.base_layer.weight', 'model.layers.1.mlp.experts_fc1.3.base_layer.bias', 'model.layers.7.mlp.experts_fc2.4.base_layer.bias', 'model.layers.19.mlp.experts_fc2.2.base_layer.bias', 'model.layers.2.mlp.experts_fc2.1.base_layer.bias', 'model.layers.19.mlp.experts_fc2.7.base_layer.weight', 'model.layers.15.mlp.experts_fc1.7.base_layer.weight', 'model.layers.13.mlp.experts_fc2.1.base_layer.weight', 'model.layers.5.mlp.experts_fc1.5.base_layer.bias', 'model.layers.23.mlp.experts_fc1.4.base_layer.bias', 'model.layers.23.mlp.experts_fc2.6.base_layer.bias', 'model.layers.9.mlp.experts_fc2.1.base_layer.weight', 'model.layers.4.mlp.experts_fc1.2.base_layer.bias', 'model.layers.12.mlp.experts_fc2.1.base_layer.bias', 'model.layers.15.mlp.experts_fc1.4.base_layer.bias', 'model.layers.15.mlp.experts_fc1.5.base_layer.bias', 'model.layers.18.mlp.experts_fc1.4.base_layer.bias', 'model.layers.1.mlp.experts_fc1.3.base_layer.weight', 'model.layers.15.mlp.experts_fc1.3.base_layer.weight', 'model.layers.19.mlp.experts_fc2.4.base_layer.bias', 'model.layers.17.mlp.experts_fc1.4.base_layer.weight', 'model.layers.6.mlp.experts_fc1.4.base_layer.bias', 'model.layers.3.mlp.experts_fc1.7.base_layer.weight', 'model.layers.19.mlp.experts_fc2.3.base_layer.bias', 'model.layers.6.mlp.experts_fc1.7.base_layer.weight', 'model.layers.13.mlp.experts_fc1.3.base_layer.bias', 'model.layers.1.mlp.experts_fc1.1.base_layer.bias', 'model.layers.12.mlp.experts_fc1.6.base_layer.bias', 'model.layers.2.mlp.experts_fc1.1.base_layer.bias', 'model.layers.20.mlp.experts_fc2.1.base_layer.weight', 'model.layers.15.mlp.experts_fc2.3.base_layer.weight', 'model.layers.10.mlp.experts_fc2.6.base_layer.weight', 'model.layers.5.mlp.experts_fc2.2.base_layer.bias', 'model.layers.20.mlp.experts_fc1.3.base_layer.weight', 'model.layers.3.mlp.experts_fc1.4.base_layer.bias', 'model.layers.14.mlp.experts_fc2.5.base_layer.weight', 'model.layers.17.mlp.experts_fc2.6.base_layer.bias', 'model.layers.8.mlp.experts_fc2.6.base_layer.bias', 'model.layers.22.mlp.experts_fc2.2.base_layer.bias', 'model.layers.0.mlp.experts_fc1.6.base_layer.weight', 'model.layers.17.mlp.experts_fc2.1.base_layer.weight', 'model.layers.11.mlp.experts_fc2.2.base_layer.bias', 'model.layers.17.mlp.experts_fc1.2.base_layer.bias', 'model.layers.18.mlp.experts_fc1.1.base_layer.weight', 'model.layers.18.mlp.experts_fc1.3.base_layer.bias', 'model.layers.6.mlp.experts_fc1.1.base_layer.bias', 'model.layers.10.mlp.experts_fc1.3.base_layer.bias', 'model.layers.2.mlp.experts_fc1.4.base_layer.weight', 'model.layers.12.mlp.experts_fc1.4.base_layer.bias', 'model.layers.5.mlp.experts_fc2.1.base_layer.weight', 'model.layers.2.mlp.experts_fc2.3.base_layer.bias', 'model.layers.10.mlp.experts_fc2.4.base_layer.weight', 'model.layers.14.mlp.experts_fc1.6.base_layer.weight', 'model.layers.14.mlp.experts_fc1.2.base_layer.bias', 'model.layers.6.mlp.experts_fc2.1.base_layer.weight', 'model.layers.9.mlp.experts_fc2.6.base_layer.bias', 'model.layers.22.mlp.experts_fc2.7.base_layer.bias', 'model.layers.8.mlp.experts_fc1.6.base_layer.weight', 'model.layers.21.mlp.experts_fc2.2.base_layer.bias', 'model.layers.1.mlp.experts_fc2.3.base_layer.weight', 'model.layers.22.mlp.experts_fc2.4.base_layer.weight', 'model.layers.22.mlp.experts_fc2.3.base_layer.weight', 'model.layers.2.mlp.experts_fc2.6.base_layer.bias', 'model.layers.0.mlp.experts_fc1.2.base_layer.bias', 'model.layers.2.mlp.experts_fc1.2.base_layer.bias', 'model.layers.1.mlp.experts_fc2.1.base_layer.weight', 'model.layers.12.mlp.experts_fc1.1.base_layer.weight', 'model.layers.21.mlp.experts_fc2.1.base_layer.bias', 'model.layers.8.mlp.experts_fc2.5.base_layer.bias', 'model.layers.11.mlp.experts_fc1.2.base_layer.bias', 'model.layers.13.mlp.experts_fc2.2.base_layer.bias', 'model.layers.12.mlp.experts_fc2.5.base_layer.weight', 'model.layers.20.mlp.experts_fc1.4.base_layer.weight', 'model.layers.0.mlp.experts_fc2.1.base_layer.bias', 'model.layers.1.mlp.experts_fc1.5.base_layer.bias', 'model.layers.12.mlp.experts_fc2.4.base_layer.weight', 'model.layers.8.mlp.experts_fc2.2.base_layer.bias', 'model.layers.12.mlp.experts_fc1.3.base_layer.weight', 'model.layers.18.mlp.experts_fc2.7.base_layer.weight', 'model.layers.9.mlp.experts_fc2.5.base_layer.bias', 'model.layers.12.mlp.experts_fc2.2.base_layer.bias', 'model.layers.1.mlp.experts_fc2.7.base_layer.bias', 'model.layers.4.mlp.experts_fc1.6.base_layer.weight', 'model.layers.14.mlp.experts_fc2.3.base_layer.weight', 'model.layers.14.mlp.experts_fc1.3.base_layer.weight', 'model.layers.4.mlp.experts_fc2.1.base_layer.weight', 'model.layers.4.mlp.experts_fc2.6.base_layer.weight', 'model.layers.15.mlp.experts_fc1.1.base_layer.bias', 'model.layers.23.mlp.experts_fc1.2.base_layer.bias', 'model.layers.21.mlp.experts_fc2.1.base_layer.weight', 'model.layers.6.mlp.experts_fc1.5.base_layer.bias', 'model.layers.12.mlp.experts_fc1.7.base_layer.bias', 'model.layers.7.mlp.experts_fc1.3.base_layer.weight', 'model.layers.16.mlp.experts_fc2.5.base_layer.bias', 'model.layers.18.mlp.experts_fc2.2.base_layer.bias', 'model.layers.4.mlp.experts_fc2.5.base_layer.bias', 'model.layers.17.mlp.experts_fc1.5.base_layer.bias', 'model.layers.11.mlp.experts_fc1.7.base_layer.weight', 'model.layers.23.mlp.experts_fc2.4.base_layer.weight', 'model.layers.21.mlp.experts_fc1.7.base_layer.weight', 'model.layers.7.mlp.experts_fc2.6.base_layer.bias', 'model.layers.10.mlp.experts_fc1.6.base_layer.bias', 'model.layers.11.mlp.experts_fc1.1.base_layer.bias', 'model.layers.8.mlp.experts_fc1.4.base_layer.bias', 'model.layers.5.mlp.experts_fc2.6.base_layer.bias', 'model.layers.22.mlp.experts_fc2.6.base_layer.weight', 'model.layers.6.mlp.experts_fc2.4.base_layer.weight', 'model.layers.23.mlp.experts_fc2.1.base_layer.weight', 'model.layers.7.mlp.experts_fc1.5.base_layer.bias', 'model.layers.9.mlp.experts_fc2.4.base_layer.bias', 'model.layers.0.mlp.experts_fc1.1.base_layer.weight', 'model.layers.19.mlp.experts_fc1.6.base_layer.weight', 'model.layers.14.mlp.experts_fc2.7.base_layer.bias', 'model.layers.14.mlp.experts_fc1.7.base_layer.weight', 'model.layers.17.mlp.experts_fc2.5.base_layer.bias', 'model.layers.22.mlp.experts_fc1.3.base_layer.bias', 'model.layers.20.mlp.experts_fc1.6.base_layer.bias', 'model.layers.0.mlp.experts_fc1.5.base_layer.weight', 'model.layers.14.mlp.experts_fc1.1.base_layer.bias', 'model.layers.13.mlp.experts_fc1.4.base_layer.bias', 'model.layers.19.mlp.experts_fc1.1.base_layer.bias', 'model.layers.3.mlp.experts_fc2.3.base_layer.bias', 'model.layers.10.mlp.experts_fc2.1.base_layer.bias', 'model.layers.4.mlp.experts_fc1.5.base_layer.weight', 'model.layers.10.mlp.experts_fc1.1.base_layer.weight', 'model.layers.1.mlp.experts_fc1.1.base_layer.weight', 'model.layers.1.mlp.experts_fc2.7.base_layer.weight', 'model.layers.10.mlp.experts_fc1.1.base_layer.bias', 'model.layers.12.mlp.experts_fc2.5.base_layer.bias', 'model.layers.9.mlp.experts_fc1.3.base_layer.weight', 'model.layers.11.mlp.experts_fc1.4.base_layer.bias', 'model.layers.8.mlp.experts_fc2.1.base_layer.bias', 'model.layers.2.mlp.experts_fc2.3.base_layer.weight', 'model.layers.15.mlp.experts_fc1.2.base_layer.weight', 'model.layers.12.mlp.experts_fc2.7.base_layer.bias', 'model.layers.7.mlp.experts_fc2.3.base_layer.weight', 'model.layers.5.mlp.experts_fc1.7.base_layer.bias', 'model.layers.18.mlp.experts_fc2.4.base_layer.weight', 'model.layers.19.mlp.experts_fc1.5.base_layer.weight', 'model.layers.15.mlp.experts_fc2.6.base_layer.bias', 'model.layers.4.mlp.experts_fc1.1.base_layer.bias', 'model.layers.13.mlp.experts_fc1.7.base_layer.bias', 'model.layers.13.mlp.experts_fc2.4.base_layer.weight', 'model.layers.6.mlp.experts_fc2.2.base_layer.bias', 'model.layers.1.mlp.experts_fc1.7.base_layer.bias', 'model.layers.11.mlp.experts_fc2.2.base_layer.weight', 'model.layers.16.mlp.experts_fc2.3.base_layer.weight', 'model.layers.5.mlp.experts_fc1.2.base_layer.bias', 'model.layers.6.mlp.experts_fc2.2.base_layer.weight', 'model.layers.18.mlp.experts_fc1.5.base_layer.bias', 'model.layers.12.mlp.experts_fc2.7.base_layer.weight', 'model.layers.9.mlp.experts_fc1.4.base_layer.bias', 'model.layers.22.mlp.experts_fc1.6.base_layer.bias', 'model.layers.5.mlp.experts_fc2.3.base_layer.weight', 'model.layers.14.mlp.experts_fc2.2.base_layer.weight', 'model.layers.5.mlp.experts_fc2.4.base_layer.weight', 'model.layers.16.mlp.experts_fc2.5.base_layer.weight', 'model.layers.3.mlp.experts_fc2.5.base_layer.bias', 'model.layers.23.mlp.experts_fc1.2.base_layer.weight', 'model.layers.1.mlp.experts_fc2.1.base_layer.bias', 'model.layers.10.mlp.experts_fc1.3.base_layer.weight', 'model.layers.16.mlp.experts_fc2.2.base_layer.weight', 'model.layers.1.mlp.experts_fc2.2.base_layer.bias', 'model.layers.15.mlp.experts_fc2.3.base_layer.bias', 'model.layers.9.mlp.experts_fc2.6.base_layer.weight', 'model.layers.9.mlp.experts_fc1.5.base_layer.weight', 'model.layers.19.mlp.experts_fc1.3.base_layer.weight', 'model.layers.5.mlp.experts_fc1.3.base_layer.bias', 'model.layers.17.mlp.experts_fc2.2.base_layer.weight', 'model.layers.21.mlp.experts_fc2.6.base_layer.weight', 'model.layers.2.mlp.experts_fc1.7.base_layer.weight', 'model.layers.5.mlp.experts_fc2.7.base_layer.weight', 'model.layers.22.mlp.experts_fc1.2.base_layer.bias', 'model.layers.7.mlp.experts_fc2.1.base_layer.weight', 'model.layers.6.mlp.experts_fc1.6.base_layer.weight', 'model.layers.14.mlp.experts_fc2.1.base_layer.bias', 'model.layers.1.mlp.experts_fc2.3.base_layer.bias', 'model.layers.10.mlp.experts_fc1.7.base_layer.weight', 'model.layers.4.mlp.experts_fc2.3.base_layer.bias', 'model.layers.17.mlp.experts_fc2.3.base_layer.bias', 'model.layers.0.mlp.experts_fc2.1.base_layer.weight', 'model.layers.2.mlp.experts_fc2.7.base_layer.weight', 'model.layers.20.mlp.experts_fc1.2.base_layer.weight', 'model.layers.23.mlp.experts_fc1.5.base_layer.weight', 'model.layers.17.mlp.experts_fc2.1.base_layer.bias', 'model.layers.13.mlp.experts_fc2.1.base_layer.bias', 'model.layers.7.mlp.experts_fc1.1.base_layer.weight', 'model.layers.23.mlp.experts_fc1.1.base_layer.bias', 'model.layers.21.mlp.experts_fc2.3.base_layer.weight', 'model.layers.5.mlp.experts_fc1.1.base_layer.bias', 'model.layers.0.mlp.experts_fc2.3.base_layer.bias', 'model.layers.7.mlp.experts_fc1.4.base_layer.weight', 'model.layers.4.mlp.experts_fc2.2.base_layer.bias', 'model.layers.21.mlp.experts_fc2.5.base_layer.weight', 'model.layers.0.mlp.experts_fc1.5.base_layer.bias', 'model.layers.12.mlp.experts_fc1.2.base_layer.bias', 'model.layers.21.mlp.experts_fc2.5.base_layer.bias', 'model.layers.7.mlp.experts_fc1.6.base_layer.bias', 'model.layers.19.mlp.experts_fc2.6.base_layer.weight', 'model.layers.15.mlp.experts_fc2.5.base_layer.bias', 'model.layers.2.mlp.experts_fc2.2.base_layer.weight', 'model.layers.9.mlp.experts_fc2.7.base_layer.weight', 'model.layers.16.mlp.experts_fc2.4.base_layer.weight', 'model.layers.4.mlp.experts_fc2.3.base_layer.weight', 'model.layers.8.mlp.experts_fc2.4.base_layer.weight', 'model.layers.14.mlp.experts_fc2.7.base_layer.weight', 'model.layers.21.mlp.experts_fc2.3.base_layer.bias', 'model.layers.23.mlp.experts_fc2.1.base_layer.bias', 'model.layers.6.mlp.experts_fc1.3.base_layer.weight', 'model.layers.7.mlp.experts_fc1.1.base_layer.bias', 'model.layers.10.mlp.experts_fc2.4.base_layer.bias', 'model.layers.11.mlp.experts_fc1.4.base_layer.weight', 'model.layers.14.mlp.experts_fc2.6.base_layer.bias', 'model.layers.0.mlp.experts_fc2.4.base_layer.bias', 'model.layers.10.mlp.experts_fc2.2.base_layer.bias', 'model.layers.16.mlp.experts_fc1.5.base_layer.weight', 'model.layers.6.mlp.experts_fc1.4.base_layer.weight', 'model.layers.9.mlp.experts_fc2.7.base_layer.bias', 'model.layers.2.mlp.experts_fc1.4.base_layer.bias', 'model.layers.7.mlp.experts_fc2.7.base_layer.bias', 'model.layers.12.mlp.experts_fc1.5.base_layer.bias', 'model.layers.5.mlp.experts_fc2.4.base_layer.bias', 'model.layers.20.mlp.experts_fc2.2.base_layer.weight', 'model.layers.22.mlp.experts_fc1.6.base_layer.weight', 'model.layers.23.mlp.experts_fc1.3.base_layer.weight', 'model.layers.5.mlp.experts_fc2.6.base_layer.weight', 'model.layers.19.mlp.experts_fc2.1.base_layer.bias', 'model.layers.22.mlp.experts_fc2.7.base_layer.weight', 'model.layers.6.mlp.experts_fc2.5.base_layer.bias', 'model.layers.17.mlp.experts_fc1.6.base_layer.bias', 'model.layers.13.mlp.experts_fc1.7.base_layer.weight', 'model.layers.19.mlp.experts_fc1.3.base_layer.bias', 'model.layers.1.mlp.experts_fc2.4.base_layer.weight', 'model.layers.2.mlp.experts_fc1.5.base_layer.weight', 'model.layers.15.mlp.experts_fc1.2.base_layer.bias', 'model.layers.21.mlp.experts_fc1.7.base_layer.bias', 'model.layers.13.mlp.experts_fc2.7.base_layer.bias', 'model.layers.16.mlp.experts_fc1.4.base_layer.bias', 'model.layers.3.mlp.experts_fc2.2.base_layer.weight', 'model.layers.6.mlp.experts_fc1.7.base_layer.bias', 'model.layers.18.mlp.experts_fc1.6.base_layer.weight', 'model.layers.10.mlp.experts_fc2.3.base_layer.weight', 'model.layers.7.mlp.experts_fc2.2.base_layer.weight', 'model.layers.10.mlp.experts_fc2.7.base_layer.bias', 'model.layers.2.mlp.experts_fc1.5.base_layer.bias', 'model.layers.11.mlp.experts_fc1.5.base_layer.bias', 'model.layers.6.mlp.experts_fc1.6.base_layer.bias', 'model.layers.15.mlp.experts_fc1.3.base_layer.bias', 'model.layers.23.mlp.experts_fc2.2.base_layer.bias', 'model.layers.17.mlp.experts_fc2.6.base_layer.weight', 'model.layers.6.mlp.experts_fc2.5.base_layer.weight', 'model.layers.23.mlp.experts_fc2.2.base_layer.weight', 'model.layers.12.mlp.experts_fc2.1.base_layer.weight', 'model.layers.8.mlp.experts_fc1.7.base_layer.bias', 'model.layers.22.mlp.experts_fc2.1.base_layer.weight', 'model.layers.5.mlp.experts_fc1.6.base_layer.weight', 'model.layers.16.mlp.experts_fc2.3.base_layer.bias', 'model.layers.20.mlp.experts_fc2.1.base_layer.bias', 'model.layers.20.mlp.experts_fc1.5.base_layer.weight', 'model.layers.16.mlp.experts_fc2.6.base_layer.bias', 'model.layers.19.mlp.experts_fc1.6.base_layer.bias', 'model.layers.9.mlp.experts_fc1.1.base_layer.bias', 'model.layers.14.mlp.experts_fc2.3.base_layer.bias', 'model.layers.16.mlp.experts_fc2.7.base_layer.bias', 'model.layers.11.mlp.experts_fc1.6.base_layer.weight', 'model.layers.5.mlp.experts_fc2.5.base_layer.bias', 'model.layers.22.mlp.experts_fc1.1.base_layer.bias', 'model.layers.9.mlp.experts_fc1.4.base_layer.weight', 'model.layers.8.mlp.experts_fc1.2.base_layer.weight', 'model.layers.13.mlp.experts_fc2.5.base_layer.weight', 'model.layers.22.mlp.experts_fc2.6.base_layer.bias', 'model.layers.17.mlp.experts_fc2.7.base_layer.bias', 'model.layers.0.mlp.experts_fc2.2.base_layer.weight', 'model.layers.2.mlp.experts_fc1.7.base_layer.bias', 'model.layers.12.mlp.experts_fc2.6.base_layer.weight', 'model.layers.3.mlp.experts_fc1.3.base_layer.weight', 'model.layers.4.mlp.experts_fc2.2.base_layer.weight', 'model.layers.11.mlp.experts_fc1.7.base_layer.bias', 'model.layers.18.mlp.experts_fc1.2.base_layer.weight', 'model.layers.0.mlp.experts_fc1.4.base_layer.bias', 'model.layers.10.mlp.experts_fc1.6.base_layer.weight', 'model.layers.3.mlp.experts_fc2.4.base_layer.bias', 'model.layers.0.mlp.experts_fc2.5.base_layer.weight', 'model.layers.20.mlp.experts_fc2.7.base_layer.weight', 'model.layers.16.mlp.experts_fc2.1.base_layer.bias', 'model.layers.23.mlp.experts_fc2.7.base_layer.bias', 'model.layers.10.mlp.experts_fc1.4.base_layer.weight', 'model.layers.23.mlp.experts_fc1.6.base_layer.bias', 'model.layers.13.mlp.experts_fc2.3.base_layer.weight', 'model.layers.7.mlp.experts_fc1.2.base_layer.weight', 'model.layers.12.mlp.experts_fc2.3.base_layer.weight', 'model.layers.19.mlp.experts_fc2.3.base_layer.weight', 'model.layers.8.mlp.experts_fc1.5.base_layer.bias', 'model.layers.16.mlp.experts_fc1.7.base_layer.weight', 'model.layers.17.mlp.experts_fc2.2.base_layer.bias', 'model.layers.16.mlp.experts_fc2.1.base_layer.weight', 'model.layers.18.mlp.experts_fc2.6.base_layer.weight', 'model.layers.20.mlp.experts_fc1.7.base_layer.bias', 'model.layers.23.mlp.experts_fc2.5.base_layer.bias', 'model.layers.21.mlp.experts_fc2.2.base_layer.weight', 'model.layers.6.mlp.experts_fc2.4.base_layer.bias', 'model.layers.22.mlp.experts_fc1.4.base_layer.bias', 'model.layers.20.mlp.experts_fc1.2.base_layer.bias', 'model.layers.9.mlp.experts_fc1.1.base_layer.weight', 'model.layers.17.mlp.experts_fc1.3.base_layer.bias', 'model.layers.8.mlp.experts_fc1.2.base_layer.bias', 'model.layers.18.mlp.experts_fc2.1.base_layer.bias', 'model.layers.2.mlp.experts_fc2.7.base_layer.bias', 'model.layers.17.mlp.experts_fc1.1.base_layer.weight', 'model.layers.15.mlp.experts_fc2.2.base_layer.bias', 'model.layers.18.mlp.experts_fc1.1.base_layer.bias', 'model.layers.13.mlp.experts_fc1.2.base_layer.bias', 'model.layers.18.mlp.experts_fc1.3.base_layer.weight', 'model.layers.20.mlp.experts_fc2.2.base_layer.bias', 'model.layers.20.mlp.experts_fc2.6.base_layer.bias', 'model.layers.7.mlp.experts_fc2.4.base_layer.weight', 'model.layers.22.mlp.experts_fc1.5.base_layer.weight', 'model.layers.3.mlp.experts_fc1.5.base_layer.weight', 'model.layers.3.mlp.experts_fc1.5.base_layer.bias', 'model.layers.0.mlp.experts_fc2.7.base_layer.bias', 'model.layers.8.mlp.experts_fc1.5.base_layer.weight', 'model.layers.5.mlp.experts_fc1.7.base_layer.weight', 'model.layers.11.mlp.experts_fc1.2.base_layer.weight', 'model.layers.3.mlp.experts_fc1.4.base_layer.weight', 'model.layers.5.mlp.experts_fc2.3.base_layer.bias', 'model.layers.12.mlp.experts_fc2.6.base_layer.bias', 'model.layers.4.mlp.experts_fc2.4.base_layer.bias', 'model.layers.19.mlp.experts_fc1.5.base_layer.bias', 'model.layers.3.mlp.experts_fc1.1.base_layer.weight', 'model.layers.1.mlp.experts_fc2.5.base_layer.bias', 'model.layers.3.mlp.experts_fc1.1.base_layer.bias', 'model.layers.15.mlp.experts_fc1.1.base_layer.weight', 'model.layers.13.mlp.experts_fc1.2.base_layer.weight', 'model.layers.20.mlp.experts_fc1.3.base_layer.bias', 'model.layers.23.mlp.experts_fc1.5.base_layer.bias', 'model.layers.4.mlp.experts_fc1.3.base_layer.weight', 'model.layers.14.mlp.experts_fc2.5.base_layer.bias', 'model.layers.21.mlp.experts_fc1.4.base_layer.bias', 'model.layers.13.mlp.experts_fc2.4.base_layer.bias', 'model.layers.19.mlp.experts_fc2.7.base_layer.bias', 'model.layers.22.mlp.experts_fc1.2.base_layer.weight', 'model.layers.16.mlp.experts_fc1.2.base_layer.weight', 'model.layers.0.mlp.experts_fc2.6.base_layer.weight', 'model.layers.2.mlp.experts_fc2.2.base_layer.bias', 'model.layers.9.mlp.experts_fc2.2.base_layer.weight', 'model.layers.15.mlp.experts_fc2.6.base_layer.weight', 'model.layers.22.mlp.experts_fc1.5.base_layer.bias', 'model.layers.4.mlp.experts_fc1.4.base_layer.weight', 'model.layers.15.mlp.experts_fc1.6.base_layer.weight', 'model.layers.17.mlp.experts_fc1.1.base_layer.bias', 'model.layers.14.mlp.experts_fc1.1.base_layer.weight', 'model.layers.13.mlp.experts_fc2.3.base_layer.bias', 'model.layers.7.mlp.experts_fc1.6.base_layer.weight', 'model.layers.5.mlp.experts_fc1.3.base_layer.weight', 'model.layers.10.mlp.experts_fc2.3.base_layer.bias', 'model.layers.5.mlp.experts_fc1.2.base_layer.weight', 'model.layers.21.mlp.experts_fc1.2.base_layer.bias', 'model.layers.13.mlp.experts_fc2.5.base_layer.bias', 'model.layers.11.mlp.experts_fc2.4.base_layer.bias', 'model.layers.15.mlp.experts_fc2.2.base_layer.weight', 'model.layers.19.mlp.experts_fc2.6.base_layer.bias', 'model.layers.2.mlp.experts_fc2.1.base_layer.weight', 'model.layers.23.mlp.experts_fc1.1.base_layer.weight', 'model.layers.1.mlp.experts_fc2.6.base_layer.bias', 'model.layers.11.mlp.experts_fc2.7.base_layer.weight', 'model.layers.23.mlp.experts_fc1.3.base_layer.bias', 'model.layers.3.mlp.experts_fc1.6.base_layer.bias', 'model.layers.2.mlp.experts_fc2.4.base_layer.bias', 'model.layers.0.mlp.experts_fc2.4.base_layer.weight', 'model.layers.12.mlp.experts_fc1.6.base_layer.weight', 'model.layers.2.mlp.experts_fc2.5.base_layer.bias', 'model.layers.0.mlp.experts_fc2.3.base_layer.weight', 'model.layers.23.mlp.experts_fc2.3.base_layer.weight', 'model.layers.19.mlp.experts_fc1.4.base_layer.weight', 'model.layers.9.mlp.experts_fc2.2.base_layer.bias', 'model.layers.2.mlp.experts_fc1.2.base_layer.weight', 'model.layers.4.mlp.experts_fc2.6.base_layer.bias', 'model.layers.10.mlp.experts_fc2.5.base_layer.bias', 'model.layers.22.mlp.experts_fc1.1.base_layer.weight', 'model.layers.4.mlp.experts_fc1.1.base_layer.weight', 'model.layers.10.mlp.experts_fc1.2.base_layer.weight', 'model.layers.15.mlp.experts_fc2.5.base_layer.weight', 'model.layers.8.mlp.experts_fc1.1.base_layer.bias', 'model.layers.5.mlp.experts_fc1.4.base_layer.bias', 'model.layers.8.mlp.experts_fc1.7.base_layer.weight', 'model.layers.13.mlp.experts_fc1.5.base_layer.bias', 'model.layers.3.mlp.experts_fc1.3.base_layer.bias', 'model.layers.4.mlp.experts_fc1.7.base_layer.weight', 'model.layers.23.mlp.experts_fc1.7.base_layer.bias', 'model.layers.12.mlp.experts_fc2.3.base_layer.bias', 'model.layers.17.mlp.experts_fc1.6.base_layer.weight', 'model.layers.20.mlp.experts_fc1.4.base_layer.bias', 'model.layers.6.mlp.experts_fc2.3.base_layer.bias', 'model.layers.3.mlp.experts_fc2.6.base_layer.weight', 'model.layers.14.mlp.experts_fc1.4.base_layer.bias', 'model.layers.3.mlp.experts_fc2.5.base_layer.weight', 'model.layers.20.mlp.experts_fc2.3.base_layer.bias', 'model.layers.22.mlp.experts_fc2.5.base_layer.weight', 'model.layers.3.mlp.experts_fc1.7.base_layer.bias', 'model.layers.18.mlp.experts_fc2.4.base_layer.bias', 'model.layers.11.mlp.experts_fc2.3.base_layer.bias', 'model.layers.8.mlp.experts_fc1.4.base_layer.weight', 'model.layers.10.mlp.experts_fc1.4.base_layer.bias', 'model.layers.10.mlp.experts_fc2.7.base_layer.weight', 'model.layers.16.mlp.experts_fc1.4.base_layer.weight', 'model.layers.16.mlp.experts_fc1.5.base_layer.bias', 'model.layers.21.mlp.experts_fc1.1.base_layer.bias', 'model.layers.19.mlp.experts_fc2.4.base_layer.weight', 'model.layers.11.mlp.experts_fc2.6.base_layer.bias', 'model.layers.0.mlp.experts_fc1.6.base_layer.bias', 'model.layers.23.mlp.experts_fc2.4.base_layer.bias', 'model.layers.1.mlp.experts_fc2.2.base_layer.weight', 'model.layers.2.mlp.experts_fc2.4.base_layer.weight', 'model.layers.20.mlp.experts_fc2.7.base_layer.bias', 'model.layers.14.mlp.experts_fc1.4.base_layer.weight', 'model.layers.21.mlp.experts_fc1.5.base_layer.bias', 'model.layers.15.mlp.experts_fc1.7.base_layer.bias', 'model.layers.2.mlp.experts_fc2.6.base_layer.weight', 'model.layers.11.mlp.experts_fc2.7.base_layer.bias', 'model.layers.11.mlp.experts_fc2.5.base_layer.weight', 'model.layers.16.mlp.experts_fc1.6.base_layer.weight', 'model.layers.21.mlp.experts_fc2.7.base_layer.bias', 'model.layers.4.mlp.experts_fc1.4.base_layer.bias', 'model.layers.16.mlp.experts_fc1.7.base_layer.bias', 'model.layers.17.mlp.experts_fc1.5.base_layer.weight', 'model.layers.20.mlp.experts_fc1.5.base_layer.bias', 'model.layers.17.mlp.experts_fc2.7.base_layer.weight', 'model.layers.23.mlp.experts_fc1.4.base_layer.weight', 'model.layers.5.mlp.experts_fc2.5.base_layer.weight', 'model.layers.21.mlp.experts_fc1.5.base_layer.weight', 'model.layers.4.mlp.experts_fc1.6.base_layer.bias', 'model.layers.3.mlp.experts_fc2.1.base_layer.weight', 'model.layers.9.mlp.experts_fc1.3.base_layer.bias', 'model.layers.14.mlp.experts_fc1.2.base_layer.weight', 'model.layers.20.mlp.experts_fc2.5.base_layer.weight', 'model.layers.8.mlp.experts_fc2.1.base_layer.weight', 'model.layers.3.mlp.experts_fc1.6.base_layer.weight', 'model.layers.14.mlp.experts_fc2.4.base_layer.weight', 'model.layers.14.mlp.experts_fc2.6.base_layer.weight', 'model.layers.21.mlp.experts_fc2.4.base_layer.weight', 'model.layers.17.mlp.experts_fc1.2.base_layer.weight', 'model.layers.6.mlp.experts_fc2.3.base_layer.weight', 'model.layers.4.mlp.experts_fc1.5.base_layer.bias', 'model.layers.12.mlp.experts_fc1.7.base_layer.weight', 'model.layers.16.mlp.experts_fc2.2.base_layer.bias', 'model.layers.20.mlp.experts_fc2.3.base_layer.weight', 'model.layers.8.mlp.experts_fc2.7.base_layer.bias', 'model.layers.5.mlp.experts_fc2.1.base_layer.bias', 'model.layers.9.mlp.experts_fc1.7.base_layer.bias', 'model.layers.10.mlp.experts_fc1.2.base_layer.bias', 'model.layers.13.mlp.experts_fc1.4.base_layer.weight', 'model.layers.0.mlp.experts_fc1.2.base_layer.weight', 'model.layers.13.mlp.experts_fc1.6.base_layer.weight', 'model.layers.1.mlp.experts_fc2.6.base_layer.weight', 'model.layers.22.mlp.experts_fc1.7.base_layer.weight', 'model.layers.0.mlp.experts_fc1.3.base_layer.bias', 'model.layers.11.mlp.experts_fc2.1.base_layer.weight', 'model.layers.7.mlp.experts_fc2.5.base_layer.bias', 'model.layers.18.mlp.experts_fc2.2.base_layer.weight', 'model.layers.14.mlp.experts_fc1.7.base_layer.bias', 'model.layers.12.mlp.experts_fc2.4.base_layer.bias', 'model.layers.20.mlp.experts_fc1.1.base_layer.weight', 'model.layers.23.mlp.experts_fc1.7.base_layer.weight', 'model.layers.18.mlp.experts_fc2.7.base_layer.bias', 'model.layers.0.mlp.experts_fc1.7.base_layer.bias', 'model.layers.12.mlp.experts_fc1.1.base_layer.bias', 'model.layers.14.mlp.experts_fc1.6.base_layer.bias', 'model.layers.1.mlp.experts_fc2.5.base_layer.weight', 'model.layers.14.mlp.experts_fc2.4.base_layer.bias', 'model.layers.19.mlp.experts_fc1.1.base_layer.weight', 'model.layers.20.mlp.experts_fc1.7.base_layer.weight', 'model.layers.11.mlp.experts_fc1.5.base_layer.weight', 'model.layers.17.mlp.experts_fc1.7.base_layer.weight', 'model.layers.9.mlp.experts_fc2.3.base_layer.bias', 'model.layers.3.mlp.experts_fc2.3.base_layer.weight', 'model.layers.5.mlp.experts_fc1.4.base_layer.weight', 'model.layers.8.mlp.experts_fc2.2.base_layer.weight', 'model.layers.14.mlp.experts_fc2.1.base_layer.weight', 'model.layers.20.mlp.experts_fc2.4.base_layer.bias', 'model.layers.10.mlp.experts_fc1.5.base_layer.weight', 'model.layers.11.mlp.experts_fc2.5.base_layer.bias', 'model.layers.0.mlp.experts_fc2.6.base_layer.bias', 'model.layers.7.mlp.experts_fc2.7.base_layer.weight', 'model.layers.23.mlp.experts_fc2.6.base_layer.weight', 'model.layers.6.mlp.experts_fc2.1.base_layer.bias', 'model.layers.9.mlp.experts_fc2.4.base_layer.weight', 'model.layers.15.mlp.experts_fc2.7.base_layer.bias', 'model.layers.1.mlp.experts_fc1.5.base_layer.weight', 'model.layers.11.mlp.experts_fc2.4.base_layer.weight', 'model.layers.17.mlp.experts_fc2.3.base_layer.weight', 'model.layers.8.mlp.experts_fc2.3.base_layer.bias', 'model.layers.20.mlp.experts_fc1.6.base_layer.weight', 'model.layers.12.mlp.experts_fc1.3.base_layer.bias', 'model.layers.6.mlp.experts_fc1.5.base_layer.weight', 'model.layers.19.mlp.experts_fc1.4.base_layer.bias', 'model.layers.12.mlp.experts_fc2.2.base_layer.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
     ]
    },
    {
     "ename": "SafetensorError",
     "evalue": "Error while serializing: IoError(Os { code: 5, kind: Uncategorized, message: \"Input/output error\" })",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSafetensorError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/cs224n-pip3/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:280\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 280\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[0;32m~/.conda/envs/cs224n-pip3/lib/python3.11/site-packages/transformers/trainer.py:1537\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/cs224n-pip3/lib/python3.11/site-packages/transformers/trainer.py:1914\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1911\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   1912\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 1914\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1916\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/.conda/envs/cs224n-pip3/lib/python3.11/site-packages/transformers/trainer.py:2279\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2276\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mstep(metrics[metric_to_check])\n\u001b[1;32m   2278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 2279\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2280\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/.conda/envs/cs224n-pip3/lib/python3.11/site-packages/transformers/trainer.py:2355\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   2353\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2354\u001b[0m     staging_output_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(run_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtmp-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2355\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstaging_output_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_internal_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_only_model:\n\u001b[1;32m   2358\u001b[0m     \u001b[38;5;66;03m# Save optimizer and scheduler\u001b[39;00m\n\u001b[1;32m   2359\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_optimizer_and_scheduler(staging_output_dir)\n",
      "File \u001b[0;32m~/.conda/envs/cs224n-pip3/lib/python3.11/site-packages/transformers/trainer.py:2849\u001b[0m, in \u001b[0;36mTrainer.save_model\u001b[0;34m(self, output_dir, _internal_call)\u001b[0m\n\u001b[1;32m   2846\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped\u001b[38;5;241m.\u001b[39msave_checkpoint(output_dir)\n\u001b[1;32m   2848\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 2849\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2851\u001b[0m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[1;32m   2852\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpush_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n",
      "File \u001b[0;32m~/.conda/envs/cs224n-pip3/lib/python3.11/site-packages/transformers/trainer.py:2909\u001b[0m, in \u001b[0;36mTrainer._save\u001b[0;34m(self, output_dir, state_dict)\u001b[0m\n\u001b[1;32m   2907\u001b[0m             torch\u001b[38;5;241m.\u001b[39msave(state_dict, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, WEIGHTS_NAME))\n\u001b[1;32m   2908\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2909\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2910\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_serialization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_safetensors\u001b[49m\n\u001b[1;32m   2911\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2914\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(output_dir)\n",
      "File \u001b[0;32m~/.conda/envs/cs224n-pip3/lib/python3.11/site-packages/transformers/modeling_utils.py:2376\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[0m\n\u001b[1;32m   2372\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m shard_file, shard \u001b[38;5;129;01min\u001b[39;00m shards\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m safe_serialization:\n\u001b[1;32m   2374\u001b[0m         \u001b[38;5;66;03m# At some point we will need to deal better with save_function (used for TPU and other distributed\u001b[39;00m\n\u001b[1;32m   2375\u001b[0m         \u001b[38;5;66;03m# joyfulness), but for now this enough.\u001b[39;00m\n\u001b[0;32m-> 2376\u001b[0m         \u001b[43msafe_save_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mformat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2377\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2378\u001b[0m         save_function(shard, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_directory, shard_file))\n",
      "File \u001b[0;32m~/.conda/envs/cs224n-pip3/lib/python3.11/site-packages/safetensors/torch.py:281\u001b[0m, in \u001b[0;36msave_file\u001b[0;34m(tensors, filename, metadata)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_file\u001b[39m(\n\u001b[1;32m    251\u001b[0m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[1;32m    252\u001b[0m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    253\u001b[0m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    254\u001b[0m ):\n\u001b[1;32m    255\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;124;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m     serialize_file(_flatten(tensors), filename, metadata\u001b[38;5;241m=\u001b[39mmetadata)\n",
      "\u001b[0;31mSafetensorError\u001b[0m: Error while serializing: IoError(Os { code: 5, kind: Uncategorized, message: \"Input/output error\" })"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "12ebaa77-b4dd-450d-973b-dc4a47465fb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.to_pickle(trainer.state, \"./tmp/trainer_state.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368f83c1-0e39-447e-983b-cc82affca68c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7747a96-c9d9-491d-9038-e5b94a804022",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ff2c832d-9ecb-4807-8dee-e081a8c18753",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import evaluate_on_nlp_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ea76f62a-c3fc-4e67-bb14-413d56422c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "110433d8-8edb-4498-acfc-81b82fe885c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a2d79529-534e-403c-b3d2-0b02a9680940",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/research/robgarct/.conda/envs/cs224n-pip3/lib/python3.11/site-packages/datasets/load.py:1429: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "100%|██████████████████| 3000/3000 [20:32<00:00,  2.43it/s]\n",
      "fatal: not a git repository (or any parent up to mount point /)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    eval_res = evaluate_on_nlp_tasks(model, tokenizer, limit=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e73b1de3-966c-46ae-b824-8dc7ab9ac4fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hellaswag': {'acc,none': 0.45666666666666667,\n",
       "  'acc_norm,none': 0.57,\n",
       "  'alias': 'hellaswag'},\n",
       " 'piqa': {'acc,none': 0.73,\n",
       "  'acc_norm,none': 0.7666666666666667,\n",
       "  'alias': 'piqa'},\n",
       " 'boolq': {'acc,none': 0.6566666666666666, 'alias': 'boolq'},\n",
       " 'winogrande': {'acc,none': 0.72, 'alias': 'winogrande'}}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_res[\"results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "61cf00bb-f362-488d-9631-dd15a723283f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since winogrande couldn't be found on the Hugging Face Hub\n",
      "2024-02-28:14:25:43,451 WARNING  [load.py:1568] Using the latest cached version of the dataset since winogrande couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'winogrande_xl' at /home/research/robgarct/.cache/huggingface/datasets/winogrande/winogrande_xl/1.1.0/85ac5b5a3b7a930e22d590176e39460400d19e41 (last modified on Thu Feb 15 14:43:29 2024).\n",
      "2024-02-28:14:25:43,453 WARNING  [cache.py:70] Found the latest cached dataset configuration 'winogrande_xl' at /home/research/robgarct/.cache/huggingface/datasets/winogrande/winogrande_xl/1.1.0/85ac5b5a3b7a930e22d590176e39460400d19e41 (last modified on Thu Feb 15 14:43:29 2024).\n",
      "Using the latest cached version of the dataset since super_glue couldn't be found on the Hugging Face Hub\n",
      "2024-02-28:14:26:14,664 WARNING  [load.py:1568] Using the latest cached version of the dataset since super_glue couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'boolq' at /home/research/robgarct/.cache/huggingface/datasets/super_glue/boolq/1.0.3/b051de3f07b5fd5ab80398a4836458db56234e24 (last modified on Thu Feb 15 14:43:34 2024).\n",
      "2024-02-28:14:26:14,666 WARNING  [cache.py:70] Found the latest cached dataset configuration 'boolq' at /home/research/robgarct/.cache/huggingface/datasets/super_glue/boolq/1.0.3/b051de3f07b5fd5ab80398a4836458db56234e24 (last modified on Thu Feb 15 14:43:34 2024).\n",
      "Using the latest cached version of the dataset since piqa couldn't be found on the Hugging Face Hub\n",
      "2024-02-28:14:26:45,024 WARNING  [load.py:1568] Using the latest cached version of the dataset since piqa couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'plain_text' at /home/research/robgarct/.cache/huggingface/datasets/piqa/plain_text/1.1.0/2e8ac2dffd59bac8c3c6714948f4c551a0848bb0 (last modified on Thu Feb 15 14:41:08 2024).\n",
      "2024-02-28:14:26:45,026 WARNING  [cache.py:70] Found the latest cached dataset configuration 'plain_text' at /home/research/robgarct/.cache/huggingface/datasets/piqa/plain_text/1.1.0/2e8ac2dffd59bac8c3c6714948f4c551a0848bb0 (last modified on Thu Feb 15 14:41:08 2024).\n",
      "Using the latest cached version of the module from /home/research/robgarct/.cache/huggingface/modules/datasets_modules/datasets/hellaswag/512a66dd8b1b1643ab4a48aa4f150d04c91680da6a4096498a5e5f799623d5ae (last modified on Thu Feb 15 14:41:12 2024) since it couldn't be found locally at hellaswag, or remotely on the Hugging Face Hub.\n",
      "2024-02-28:14:27:15,562 WARNING  [load.py:1537] Using the latest cached version of the module from /home/research/robgarct/.cache/huggingface/modules/datasets_modules/datasets/hellaswag/512a66dd8b1b1643ab4a48aa4f150d04c91680da6a4096498a5e5f799623d5ae (last modified on Thu Feb 15 14:41:12 2024) since it couldn't be found locally at hellaswag, or remotely on the Hugging Face Hub.\n",
      " 35%|██████           | 3540/10000 [25:11<44:48,  2.40it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_res = evaluate_on_nlp_tasks(model, tokenizer, limit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b1d57837-716d-4aed-b695-f39210ec33f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hellaswag': {'acc,none': 0.428,\n",
       "  'acc_norm,none': 0.545,\n",
       "  'alias': 'hellaswag'},\n",
       " 'piqa': {'acc,none': 0.743, 'acc_norm,none': 0.751, 'alias': 'piqa'},\n",
       " 'boolq': {'acc,none': 0.649, 'alias': 'boolq'},\n",
       " 'winogrande': {'acc,none': 0.691, 'alias': 'winogrande'}}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_res[\"results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a592962c-bce7-41cc-b8af-1e5446f8a438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0764816e-9495-4911-a415-d91ed9e33cf5",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b94d3586-1954-40cb-8e05-7f501ef932cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b11d6a89-24aa-4872-83bc-a07d66f4be53",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./tmp/model_state_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e2612c-7683-4000-8cc8-0b8e378312f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5a094e2-ade6-4b62-9727-ac6d81969a78",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d361be45-fb3b-4655-b957-0a151f82460f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "52f079f0-584a-471f-a9e4-f8ac3aa9168d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-7.8082e-03, -1.4024e-02, -6.4943e-03,  ...,  9.5486e-03,\n",
       "         -4.2409e-03, -1.8877e-02],\n",
       "        [-6.5831e-03, -1.5125e-02, -7.5041e-03,  ..., -1.1292e-02,\n",
       "         -1.7146e-02, -1.2725e-03],\n",
       "        [-1.4931e-02, -1.4116e-02, -2.2513e-02,  ...,  9.7545e-05,\n",
       "          1.6355e-02, -1.1077e-02],\n",
       "        ...,\n",
       "        [ 9.7663e-03, -1.4400e-02, -1.8612e-02,  ...,  2.1575e-02,\n",
       "          1.2617e-03, -1.5609e-02],\n",
       "        [-1.0753e-02,  1.6160e-02, -1.1062e-02,  ..., -1.1271e-02,\n",
       "         -1.5020e-02,  2.0665e-02],\n",
       "        [ 9.4092e-03,  1.4645e-02, -1.2627e-02,  ...,  1.4613e-02,\n",
       "         -1.5689e-02,  1.8180e-02]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].mlp.experts_fc1[0].lora_A.default.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "68be6781-0c8a-4650-b5d4-334e5f698053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0465, -0.0024,  0.0215,  ..., -0.0051,  0.0354, -0.0269],\n",
       "        [ 0.0023, -0.0248, -0.0047,  ..., -0.0262,  0.0047, -0.0208],\n",
       "        [-0.0111, -0.0166,  0.0031,  ...,  0.0067, -0.0110,  0.0097],\n",
       "        ...,\n",
       "        [-0.0154, -0.0031, -0.0254,  ..., -0.0294, -0.0067,  0.0031],\n",
       "        [ 0.0088,  0.0296, -0.0017,  ...,  0.0020, -0.0049,  0.0049],\n",
       "        [ 0.0237,  0.0075,  0.0305,  ...,  0.0317,  0.0244, -0.0186]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[1].mlp.experts_fc1[0].lora_A.default.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1634565c-bc4d-41dc-8650-9b20e5efef83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a46d90a3-be05-4f1a-8a6f-3a4474b31c87",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiForCausalLM(\n",
       "  (model): PhiModel(\n",
       "    (embed_tokens): EmbeddingTokenIdxTracker(\n",
       "      (embed): Embedding(51200, 2048)\n",
       "    )\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x PhiDecoderLayer(\n",
       "        (self_attn): PhiAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (rotary_emb): PhiRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Experts(\n",
       "          (activation_fn): NewGELUActivation()\n",
       "          (cluster_router): ClusterRouter()\n",
       "          (experts_fc1): ModuleList(\n",
       "            (0-7): 8 x lora.Linear(\n",
       "              (base_layer): Linear(in_features=2048, out_features=6554, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=6554, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "            )\n",
       "          )\n",
       "          (experts_fc2): ModuleList(\n",
       "            (0-7): 8 x lora.Linear(\n",
       "              (base_layer): Linear(in_features=6554, out_features=2048, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=6554, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (final_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=51200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ef3116ff-9c71-4a91-b701-d69ebc41182b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'PreTrainedModel.enable_input_require_grads.<locals>.make_inputs_require_grads'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmoe-v1.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/cs224n-pip3/lib/python3.11/site-packages/pandas/io/pickle.py:113\u001b[0m, in \u001b[0;36mto_pickle\u001b[0;34m(obj, filepath_or_buffer, compression, protocol, storage_options)\u001b[0m\n\u001b[1;32m    103\u001b[0m     protocol \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mHIGHEST_PROTOCOL\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[1;32m    106\u001b[0m     filepath_or_buffer,\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    111\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# letting pickle write directly to the buffer is more memory-efficient\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m     \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't pickle local object 'PreTrainedModel.enable_input_require_grads.<locals>.make_inputs_require_grads'"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    pd.to_pickle(model, \"moe-v1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eaea87-d19c-4eda-980e-7ed9546ca1e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83340de0-5185-471d-b7e2-40bfc8fe49a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb021c1-13a2-4925-8c1c-925deb8fb055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d25c98a-0b8a-44d5-843e-f519eb4d3c24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c582f8ff-54fe-481f-b4da-86292f1f0ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['results', 'configs', 'versions', 'n-shot', 'config', 'git_hash']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(eval_res.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d93e92-f096-4301-8811-5ef869c801a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1215ab07-7e15-48fa-a668-3afb2cb36418",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
